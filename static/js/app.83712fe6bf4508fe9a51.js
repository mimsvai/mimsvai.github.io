webpackJsonp([0],{"2FDy":function(e,n){},"7w3D":function(e,n){},AhPL:function(e,n){},NHnr:function(e,n,t){"use strict";Object.defineProperty(n,"__esModule",{value:!0});var i=t("7+uW"),a={render:function(){var e=this.$createElement,n=this._self._c||e;return n("div",{attrs:{id:"app"}},[n("transition",{attrs:{name:"page",mode:"out-in"}},[n("router-view")],1)],1)},staticRenderFns:[]};var s=t("VU/8")({name:"App",components:{}},a,!1,function(e){t("t5nL")},null,null).exports,o=t("/ocq"),r={props:["year"],data:function(){return{iconSrc:"https://caepo.org/images/menu-logo.svg",title:{2023:"MIMSVAI 2023",2022:"MIMSVAI 2022",2021:"MIMSVAI 2021"},menuList:{2023:[{name:"ABOUT",id:"ABOUT"},{name:"CALL FOR PAPERS",id:"PAPERS"},{name:"KEYNOTES",id:"KEYNOTES"},{name:"AGENDA",id:"AGENDA"},{name:"ORGANIZER",id:"ORGANIZER"},{name:"VENUE",id:"VENUE"}],2022:[{name:"ABOUT",id:"ABOUT"},{name:"CALL FOR PAPERS",id:"PAPERS"},{name:"KEYNOTES",id:"KEYNOTES"},{name:"AGENDA",id:"AGENDA"},{name:"ORGANIZER",id:"ORGANIZER"},{name:"VENUE",id:"VENUE"}],2021:[{name:"ABOUT",id:"ABOUT"},{name:"CALL FOR PAPERS",id:"PAPERS"},{name:"KEYNOTES",id:"KEYNOTES"},{name:"PANEL DISCUSSION",id:"PANEL"},{name:"AGENDA",id:"AGENDA"},{name:"ORGANIZER",id:"ORGANIZER"},{name:"VENUE",id:"VENUE"}]},drawerStatus:!1,menuclassList:{menuIconContainer:!0,change:!1},name:"Oppps",isTop:!0,totopImg:"https://mimsvai.github.io/static/imgs/toTopIcon.png"}},watch:{$route:function(){this.name=this.$route.params.PID}},mounted:function(){this.name=this.$route.params.PID,window.addEventListener("scroll",this.handleScroll)},methods:{opentheDrawer:function(){this.drawerStatus=!this.drawerStatus,this.menuclassList.change=!this.menuclassList.change},handleScroll:function(e,n){window.scrollY>100?this.isTop=!1:this.isTop=!0},scroll_to:function(e){var n=document.getElementById(e).offsetTop;window.scrollTo(0,n)}}},l={render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{attrs:{id:"menuContainer"}},[t("div",{class:{"lego-top":e.isTop},staticStyle:{"text-decoration":"none"},attrs:{id:"Lego"}},[t("h1",[e._v(e._s(e.title[e.year]))])]),t("h1",{staticClass:"TitleB"},[e._v(e._s(e.name))]),e._l(e.menuList[e.year],function(n){return t("div",{staticClass:"menuItem",class:{"menu-top":e.isTop},on:{click:function(t){return e.scroll_to(n.id)}}},[e._v(e._s(n.name)),t("div",{attrs:{id:"underline"}})])}),e.drawerStatus?t("div",{attrs:{id:"drawer"}},e._l(e.menuList[e.year],function(n,i){return t("div",{staticClass:"drawerItem",on:{click:function(n){return e.opentheDrawer()}}},[t("a",{attrs:{href:"#"+n.name}},[e._v(e._s(n.name)),t("div",{attrs:{id:"underline"}})])])}),0):e._e(),t("div",{staticClass:"menuIcon",class:e.menuclassList,on:{click:function(n){return e.opentheDrawer()}}},[t("div",{staticClass:"menuIconBar1"}),t("div",{staticClass:"menuIconBar2"}),t("div",{staticClass:"menuIconBar3"})]),t("a",{directives:[{name:"show",rawName:"v-show",value:!e.isTop,expression:"!isTop"}],staticClass:"toTopIcon",style:{"background-image":"url("+e.totopImg+")"},attrs:{href:"#"}})],2)},staticRenderFns:[]};var c={render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"introContainer"},[t("div",{staticClass:"introImg",style:{"background-image":"url("+e.intro[e.year].src+")"}},[t("div",{staticClass:"introMask"},[t("div",{staticClass:"introText"},[e._v(e._s(e.intro[e.year].text1))]),t("h1",{staticClass:"TitleU"},[e._v(e._s(e.intro[e.year].name))]),t("div",{staticClass:"introText",domProps:{innerHTML:e._s(e.intro[e.year].text2)}})])])])},staticRenderFns:[]};var h={render:function(){var e=this.$createElement,n=this._self._c||e;return n("div",{staticClass:"contentContainer"},[n("h1",{staticClass:"TitleU"},[this._v(this._s(this.name))]),n("div",{staticClass:"HTMLContainer",domProps:{innerHTML:this._s(this.body[this.year])}})])},staticRenderFns:[]};var p={render:function(){var e=this.$createElement,n=this._self._c||e;return n("div",{staticClass:"contentContainer"},[n("h1",{staticClass:"TitleU"},[this._v(this._s(this.name))]),n("p"),n("div",{staticClass:"HTMLContainer",domProps:{innerHTML:this._s(this.body[this.year])}})])},staticRenderFns:[]};var d={render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"sraffPage"},[t("h1",{staticClass:"TitleU"},[e._v(e._s(e.name))]),e._l(e.data[e.year],function(n){return t("div",{staticClass:"staffContainer"},[t("h1",{staticClass:"chairTitle"},[e._v(e._s(n.title))]),t("div",{staticClass:"staffListContainer"},e._l(n.staff,function(n){return t("div",{staticClass:"staffListContentBlock",attrs:{"data-aos":"fade-up","data-aos-duration":"2000"}},[""!=n.href?t("a",{staticClass:"staffListName",staticStyle:{color:"#e8b279"},attrs:{href:n.href,target:"_blank"}},[e._v(e._s(n.name))]):e._e(),""==n.href?t("div",{staticClass:"staffListName"},[e._v(e._s(n.name))]):e._e(),t("div",{staticClass:"staffListTitle"},[e._v(e._s(n.agency))])])}),0)])})],2)},staticRenderFns:[]};var u={render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"contentContainer"},[t("h1",{staticClass:"TitleU"},[e._v(e._s(e.name))]),t("p",[e._v(e._s(e.discription[e.year]))]),t("table",[t("thead"),t("tr",{staticClass:"headRow"},e._l(e.thList,function(n,i){return t("th",{key:i,staticClass:"headLabel"},[e._v(e._s(n))])}),0),t("tbody"),e._l(e.rowList[e.year],function(n,i){return t("tr",{key:i},[t("td",[t("strong",[e._v(e._s(n.time))])]),t("td",{staticStyle:{"white-space":"pre-wrap"}},[t("strong",[e._v(e._s(n.content))]),n.more?t("p",{staticClass:"tableCotent"},[e._v(e._s(n.more))]):e._e()])])})],2)])},staticRenderFns:[]};var m={render:function(){var e=this.$createElement,n=this._self._c||e;return n("div",{staticClass:"contentContainer"},[n("h1",{staticClass:"TitleU"},[this._v(this._s(this.name))]),n("div",{staticClass:"HTMLContainer",domProps:{innerHTML:this._s(this.body[this.year])}})])},staticRenderFns:[]};var g={render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"contentContainer"},[t("h1",{staticClass:"TitleU"},[e._v(e._s(e.name))]),t("h2",{staticClass:"subTitle"},[e._v("Keynote Speakers")]),e._l(e.speakers[e.year],function(n){return t("div",{staticClass:"speakerContainer",attrs:{"data-aos":"fade-up","data-aos-duration":"1000"}},[t("div",{staticClass:"speakerImg",style:{"background-image":"url("+n.img+")"}}),t("div",{staticClass:"staffListContentBlock"},[t("h2",{staticClass:"staffListName"},[e._v(e._s(n.name))]),t("div",{staticClass:"staffListTitle",domProps:{innerHTML:e._s(n.agency)}}),t("h3",[e._v(e._s(n.keynote)+" :")]),t("p",[e._v(e._s(n.content))]),""!=n.summary?t("h3",[e._v("Summary :")]):e._e(),t("div",{staticClass:"HTMLContainer",domProps:{innerHTML:e._s(n.summary)}}),""!=n.biography?t("h3",[e._v("Biography :")]):e._e(),t("div",{staticClass:"HTMLContainer",domProps:{innerHTML:e._s(n.biography)}})])])})],2)},staticRenderFns:[]};var f={render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"contentContainer"},[t("h1",{staticClass:"TitleU"},[e._v(e._s(e.name))]),t("div",{staticClass:"textSize",attrs:{"data-aos":"fade-up"}},[e._l(e.panels[e.year].texts,function(n){return t("p",{staticClass:"textSize"},[e._v(e._s(n))])}),t("ul",e._l(e.panels[e.year].lis,function(n){return t("li",{staticClass:"textSize"},[e._v(e._s(n))])}),0)],2)])},staticRenderFns:[]};var y={name:"HomePage",components:{MenuM:t("VU/8")(r,l,!1,function(e){t("7w3D")},null,null).exports,Intro:t("VU/8")({props:["year"],data:function(){return{intro:{2023:{name:"The Third Workshop on Multiple Input Modalities and Sensations for VR/AR Interactions (MIMSVAI 2023)",src:"https://mimsvai.github.io/static/imgs/banner.jpg",text1:"October 9th, 2023, in Cancún, Mexico",text2:'<span>Workshop in conjunction with </span><a href="https://www.ubicomp.org/ubicomp2023/" target="blank" style="color: #e89700;">UbiComp 2023↗︎</a>'},2022:{name:"The Second Workshop on Multiple Input Modalities and Sensations for VR/AR Interactions (MIMSVAI 2022)",src:"https://mimsvai.github.io/static/imgs/banner.jpg",text1:"September 11, 2022, organised virtually",text2:'<span>Workshop in conjunction with </span><a href="https://www.ubicomp.org/ubicomp2022/" target="blank" style="color: #e89700;">UbiComp 2022↗︎</a>'},2021:{name:"The First Workshop on Multiple Input Modalities and Sensations for VR/AR Interactions (MIMSVAI 2021)",src:"https://mimsvai.github.io/static/imgs/banner.jpg",text1:"September 26, 2021, organised virtually",text2:'<span>Workshop in conjunction with </span><a href="https://www.ubicomp.org/ubicomp2021/" target="blank" style="color: #e89700;">UbiComp 2021↗︎</a>'}}}},methods:{}},c,!1,function(e){t("tXKz")},"data-v-3f2ee5cc",null).exports,About:t("VU/8")({props:["year"],data:function(){return{name:"ABOUT THE WORKSHOP",body:{2023:'<p dir="ltr" style="line-height: 1.7; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 19px; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Immersion in a cohesive virtual reality and/or augmented reality (VR/AR) environment depends on multiple realistic sensations, whereas the ability to interact with the system depends on an unobtrusive interface and intuitive input modalities. Rapid technological advances have greatly improved the user experience; however, VR/AR system developers are striving to further enhance the realism of VR/AR applications by incorporating multiple sensory inputs, including visual systems of greater sophistication, haptic feedback, auditory cues, and olfactory sensations. Developers are also seeking to employ a diversity of input modalities, such as eye gaze, voice, and gesture to enable seamless interactions with the virtual/augmented environment. Nonetheless, VR/AR developers are struggling to provide users with an experience that is as realistic and immersive as those experienced in the real world.</span></p><br><p dir="ltr" style="line-height: 1.7; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 19px; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">The Multiple Input Modalities and Sensations for VR/AR Interactions (MIMSVAI 2023) workshop will be a one-day face-to-face gathering of researchers in the fields of ubiquitous computing (UbiComp) and VR/AR. Note that attendees planning to attend the workshop in Cancún Mexico on October 9th, 2023 will be expected to register for the UbiComp/ISWC 2023 main conference. Our initial efforts will be aimed at developing research momentum in the UbiComp community by calling for papers on the design of input modalities and sensory feedback mechanisms aimed at enhancing the immersive quality of the VR/AR experience and facilitating computer-user interactions. We arrange sessions for each accepted paper to be presented in the workshop. At the end of each session, we will assign session chairs to mediate the discussion to distill motivated ideas and findings to the participants. Besides the regular sessions, we will invite one expert for VR/AR interactions to give talks in the morning. We hope the keynote presentation can bring in more interesting perspectives regarding VR/AR-related researches. In this year, we further incorporate discussion sessions in the afternoon to engage workshop attendees in productive brainstorming and discussion to generate more potential challenges or ideas to enable more immersive VR/AR experiences. Finally, we will deliver the closing remarks and give the best paper award to the best paper presented at the MIMSVAI 2023 workshop to acknowledge and encourage excellence in research. The awardee will be presented with a certificate at the workshop.</span></p>',2022:'<p dir="ltr" style="line-height: 1.7; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 19px; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">In order to gather research momentum from the UbiComp community, we call for papers discussing how to design or incorporate different input modalities or sensations for enabling a more immersive VR/AR experience. To reduce long-haul travel following the pandemic, MIMSVAI 2022 will be a virtual workshop held on September 11th for on-site as well as online attendees, following a half-day format. Those expecting to attend the UbiComp/ISWC 2022 conference may also register during this workshop. We arrange sessions for each accepted paper to be presented in the workshop. At the end of each session, we will assign session chairs to mediate the discussion to distill motivated ideas and findings to the participants. &nbsp;</span></p><br><p dir="ltr" style="line-height: 1.7; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 19px; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Besides the regular sessions, we will invite one expert for VR/AR interactions to give talks in the morning. We hope the keynote presentation can bring in more interesting perspectives regarding VR/AR-related researches. Finally, we will deliver the closing remarks and give the best paper award to the best paper presented at the MIMSVAI 2022 workshop to acknowledge and encourage excellence in research. The awardee will be presented with a certificate at the workshop.</span></p>',2021:'<p dir="ltr" style="line-height: 1.7; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 19px; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">In order to gather research momentum from the UbiComp community, we call for papers discussing how to design or incorporate different input modalities or sensations for enabling a more immersive VR/AR experience. We arrange sessions for each accepted paper to be presented in the workshop. At the end of each session, we will assign session chairs to mediate the discussion to distill motivated ideas and findings to the participants.&nbsp;</span></p><br><p dir="ltr" style="line-height: 1.7; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 19px; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Besides the regular sessions, we will invite two experts for VR/AR interactions to give talks in the morning and the afternoon, respectively. We hope these two keynote presentations can bring in more interesting perspectives regarding VR/AR-related researches. Finally, a panel discussion will be held in the late afternoon. We will invite 3-5 panelists from both academia and industrial sides to share their valuable experiences related to current research or development trends of VR/AR applications. </span></p>'}}},methods:{}},h,!1,function(e){t("dXnZ")},"data-v-02095969",null).exports,Paper:t("VU/8")({props:["year"],data:function(){return{name:"CALL FOR PAPERS",body:{2023:'<p>The increasing popularity of VR/AR technology has led to the emergence of numerous VR/AR applications that appeal to new users. To enhance the level of perceived realism, alternative input modalities and multiple realistic sensations need to be developed and integrated coherently for users to interact with these technologies. Although these developments have the potential to create more immersive VR/AR experiences, the lack of robust and intuitive interaction interfaces and realistic sensations limits the level of users\' acceptance in various application areas. This workshop aims to address the challenges and opportunities of designing a higher coherence between different input modalities and sensations to offer more engaging VR/AR experiences. It provides a platform for researchers from both UbiComp and VR/AR fields to collaborate and explore alternative input modalities and sensations for VR/AR interactions.</p>\n\n                <h1>Topics</h1>\n\n                <p>Papers may include some of the topics, but are not limited to:</p>\n\n                <ul>\n                    <li>2D/3D and volumetric display and projection technology</li>\n                    <li>Immersive analytics and visualization</li>\n                    <li>Modeling and simulation</li>\n                    <li>Multimodal capturing and reconstruction</li>\n                    <li>Scene description and management issues</li>\n                    <li>Storytelling</li>\n                    <li>Tracking and sensing</li>\n                    <li>Audio interfaces, sound rendering, spatialized audio, auditory perception and psychoacoustics</li>\n                    <li>Embodied agents, virtual humans and (self-)avatars</li>\n                    <li>Haptic and tactile interfaces, wearable haptics, passive haptics, pseudo haptics, other touch-based UI</li>\n                    <li>Mediated and diminished reality</li>\n                    <li>Multimodal input and output</li>\n                    <li>Multisensory rendering, registration, and synchronization</li>\n                    <li>Perception and cognition</li>\n                    <li>Presence, body ownership, and agency</li>\n                    <li>Teleoperation and telepresence</li>\n                    <li>3D user interaction</li>\n                    <li>3D UI metaphors</li>\n                    <li>Collaborative interactions</li>\n                    <li>Ethical issues</li>\n                    <li>Human factors and ergonomics</li>\n                    <li>Input devices</li>\n                    <li>Locomotion and navigation</li>\n                    <li>Multimodal/cross-modal interaction and perception</li>\n                    <li>Non-fatiguing 3D UIs</li>\n                    <li>Non-visual interfaces (such as olfactory)</li>\n                    <li>Touch, tangible and gesture interfaces</li>\n                    <li>Usage research, evaluation methods and empirical studies</li>\n                    <li>Domain-specific VR/AR applications.</li>\n                    <li>Design process, platform/tools, or libraries used to incorporate input modalities or sensations for VR/AR interactions or studying their use</li>\n                </ul>\n\n                <h1>Important Dates</h1>\n\n                <p>Paper submission: 23:59 (AoE), <s>June 2, 2023</s> <s>June 9, 2023</s> June 16, 2023 <span style="color:#e74c3c">(extended and final)</span></p>\n\n                <p>Paper notification: 23:59 (AoE), <s>June 30, 2023</s> July 8, 2023 <span style="color:#e74c3c">(extended)</span></p>\n\n                <p>Camera-ready: 23:59 (AoE), July 31, 2023</p>\n\n                <p>Workshop: October 9th, 2023</p>\n\n                <h1>Best Paper Award</h1>\n\n                <p>The Best Paper Award is given to the best paper presented at the MIMSVAI 2023 workshop, to acknowledge and encourage excellence in research. The awardee will be presented with a certificate at the workshop.</p>\n\n                <h2>Submission Guidelines</h2>\n\n                <p>Submissions should be made electronically through the&nbsp;<a href="https://new.precisionconference.com/" target="blank">Online Submission System (PCS)</a><strong>. </strong>Please select &ldquo;SIGCHI&rdquo; as Society, &ldquo;Ubicomp/ISWC 2023&rdquo; as Conference/Journal, and &ldquo;UbiComp/ISWC 2023 Workshop MIMSVAI&rdquo; as the track on the submission page.<br />\n                <br />\n                <strong><span style="color:#e74c3c">All papers need to be anonymized</span>.</strong> Please submit papers with a maximum length of 5 pages (4-page + 1 references) in ACM SIGCHI sigconf template with 2 columns (<a href="https://www.acm.org/publications/proceedings-template">https://www.acm.org/publications/proceedings-template</a>). Please contact us (<a href="mailto:ubicomp.mimsvai@gmail.com">ubicomp.mimsvai@gmail.com</a>) if you have any problems when preparing your submissions.</p>\n\n                <p>At least one author of each accepted paper needs to register for the conference and the workshop itself. During the workshop, each paper will be presented briefly by one of the authors. And, the Best Paper Award will be given to the best paper presented at the MIMSVAI 2023 workshop, to acknowledge and encourage excellence in research. The awardee will be presented with a certificate at the workshop.</p>\n\n                <p>The accepted papers will be published in the UbiComp/ISWC Adjunct Proceedings, which will be included in the ACM Digital Library as part of the UbiComp conference supplemental proceedings.</p>\n                ',2022:'<p>With the advance of VR/AR technology, more and more VR/AR applications are emerging and have been popular among new users. Interacting with virtual reality and augmented reality technologies will require the development of alternative input modalities as well as coherent integration of multiple realistic sensations to increase the level of perceived realism. These developments will create a more immersive VR/AR experience. However, a lack of robust and intuitive interaction interfaces and realistic sensations hinders users&#39; experience for achieving a fascinating acceptance in various application areas of VR/AR interactions. This workshop discusses the challenges and applications of designing a higher coherence between different input modalities and sensations to offer more engaging VR/AR experiences, which can create opportunities for the researchers from both UbiComp and VR/AR fields to jointly discuss and brainstorm alternative input modalities and sensations for VR/AR interactions.</p>\n\n                <h1>Topics</h1>\n\n                <p>Papers may include, but not be limited to, topics such as:</p>\n\n                <ul>\n                    <li>2D/3D and volumetric display and projection technology</li>\n                    <li>Immersive analytics and visualization</li>\n                    <li>Modeling and simulation</li>\n                    <li>Multimodal capturing and reconstruction</li>\n                    <li>Scene description and management issues</li>\n                    <li>Storytelling</li>\n                    <li>Tracking and sensing</li>\n                    <li>Audio interfaces, sound rendering, spatialized audio, auditory perception and psychoacoustics</li>\n                    <li>Embodied agents, virtual humans and (self-)avatars</li>\n                    <li>Haptic and tactile interfaces, wearable haptics, passive haptics, pseudo haptics, other touch-based UI</li>\n                    <li>Mediated and diminished reality</li>\n                    <li>Multimodal input and output</li>\n                    <li>Multisensory rendering, registration, and synchronization</li>\n                    <li>Perception and cognition</li>\n                    <li>Presence, body ownership, and agency</li>\n                    <li>Teleoperation and telepresence</li>\n                    <li>3D user interaction</li>\n                    <li>3D UI metaphors</li>\n                    <li>Collaborative interactions</li>\n                    <li>Ethical issues</li>\n                    <li>Human factors and ergonomics</li>\n                    <li>Input devices</li>\n                    <li>Locomotion and navigation</li>\n                    <li>Multimodal/cross-modal interaction and perception</li>\n                    <li>Non-fatiguing 3D UIs</li>\n                    <li>Non-visual interfaces (such as olfactory)</li>\n                    <li>Touch, tangible and gesture interfaces</li>\n                    <li>Usage research, evaluation methods and empirical studies</li>\n                    <li>Domain-specific VR/AR applications.</li>\n                    <li>Design process, platform/tools, or libraries used to incorporate input modalities or sensations for VR/AR interactions or studying their use</li>\n                </ul>\n\n                <h1>Important Dates</h1>\n\n                <p>Paper submission: 23:59 (AoE), <s>July 29, 2022</s> August 5, 2022&nbsp;</p>\n\n                <p>Paper notification: 23:59 (AoE), <s>August 21, 2022</s> August 26, 2022</p>\n\n                <p>Camera-ready: 23:59 (AoE), September 2, 2022</p>\n\n                <p>Workshop: September 11, 2022</p>\n\n                <h1>Best Paper Award</h1>\n\n                <p>The Best Paper Award is given to the best paper presented at the MIMSVAI 2022 workshop, to acknowledge and encourage excellence in research. The awardee will be presented with a certificate at the workshop.</p>\n\n                <h2>Submission Guidelines</h2>\n\n                <p>Submissions should be made electronically through the&nbsp;<a href="https://new.precisionconference.com/" target="blank">Online Submission System (PCS)</a><strong>. </strong>Please select &ldquo;SIGCHI&rdquo; as Society, &ldquo;Ubicomp/ISWC 2022&rdquo; as Conference/Journal, and &ldquo;UbiComp/ISWC 2022 Workshop MIMSVAI&rdquo; as the track on the submission page.<br />\n                <br />\n                <strong><span style="color:#e74c3c">All papers need to be anonymized</span>.</strong> Please submit papers with a maximum length of 5 pages (4-page + 1 references) in ACM SIGCHI sigconf template with 2 columns (<a href="https://www.acm.org/publications/proceedings-template">https://www.acm.org/publications/proceedings-template</a>). Please contact us (<a href="mailto:ubicomp.mimsvai@gmail.com">ubicomp.mimsvai@gmail.com</a>) if you have any problems when preparing your submissions.</p>\n\n                <p>At least one author of each accepted paper needs to register for the conference and the workshop itself. During the workshop, each paper will be presented briefly by one of the authors. And, the Best Paper Award will be given to the best paper presented at the MIMSVAI 2022 workshop, to acknowledge and encourage excellence in research. The awardee will be presented with a certificate at the workshop.</p>\n\n                <p>The accepted papers will be published in the UbiComp/ISWC Adjunct Proceedings, which will be included in the ACM Digital Library as part of the UbiComp conference supplemental proceedings.</p>\n                ',2021:'<p dir="ltr">With the advance of VR/AR technology, more and more VR/AR&nbsp;applications are emerging and have been popular among new users.&nbsp;To facilitate user interaction with the systems, multi-modal input&nbsp;modalities are required. Furthermore, coherent integration of&nbsp;multiple realistic sensations can increase the level of realism, and&nbsp;therefore creates a truly immersive VR/AR experience. However,&nbsp;a lack of robust and intuitive interaction interfaces and realistic&nbsp;sensations hinders users&rsquo; experience for achieving a fascinating&nbsp;acceptance in various application areas of VR/AR interactions. This&nbsp;workshop discusses the challenges and applications of designing a&nbsp;higher coherence between the different input modalities and sensations&nbsp;to provide more engaging VR/AR experiences by leveraging&nbsp;the advances of both VR/AR software and hardware to improve user experience.</p>\n\n                <h1 dir="ltr">Topics</h1>\n\n                <p>Papers may include, but not be limited to, topics such as:</p>\n\n                <ul>\n                    <li>2D/3D and volumetric display and projection technology</li>\n                    <li>Immersive analytics and visualization</li>\n                    <li>Modeling and simulation</li>\n                    <li>Multimodal capturing and reconstruction</li>\n                    <li>Scene description and management issues</li>\n                    <li>Storytelling</li>\n                    <li>Tracking and sensing</li>\n                    <li>Audio interfaces, sound rendering, spatialized audio, auditory perception and psychoacoustics</li>\n                    <li>Embodied agents, virtual humans and (self-)avatars</li>\n                    <li>Haptic and tactile interfaces, wearable haptics, passive haptics, pseudo haptics, other touch-based UI</li>\n                    <li>Mediated and diminished reality</li>\n                    <li>Multimodal input and output</li>\n                    <li>Multisensory rendering, registration, and synchronization</li>\n                    <li>Perception and cognition</li>\n                    <li>Presence, body ownership, and agency</li>\n                    <li>Teleoperation and telepresence</li>\n                    <li>3D user interaction</li>\n                    <li>3D UI metaphors</li>\n                    <li>Collaborative interactions</li>\n                    <li>Ethical issues</li>\n                    <li>Human factors and ergonomics</li>\n                    <li>Input devices</li>\n                    <li>Locomotion and navigation</li>\n                    <li>Multimodal/cross-modal interaction and perception</li>\n                    <li>Non-fatiguing 3D UIs</li>\n                    <li>Non-visual interfaces (such as olfactory)</li>\n                    <li>Touch, tangible and gesture interfaces</li>\n                    <li>Usage research, evaluation methods and empirical studies</li>\n                    <li>Domain-specific VR/AR applications.</li>\n                    <li>Design process, platform/tools, or libraries used to incorporate input modalities or sensations for VR/AR interactions or studying their use</li>\n                </ul>\n\n                <h1 dir="ltr">Important Dates</h1>\n\n                <p dir="ltr">Paper submission: 23:59 (AoE), <s>June 15, 2021&nbsp;June 22, 2021</s> June 30, 2021 <span style="color:#e74c3c">(extended and final)</span></p>\n\n                <p dir="ltr">Paper notification: 23:59 (AoE), July 15, 2021</p>\n\n                <p dir="ltr">Camera-ready: 23:59 (AoE), <s>July 31, 2021 Aug. 6, 2021</s> Aug. 10, 2021<span style="color:#e74c3c"> (extended)</span></p>\n\n                <p dir="ltr">Workshop: September 26, 2021</p>\n\n                <h1 dir="ltr">Best Paper Award</h1>\n\n                <p dir="ltr">The Best Paper Award is given to the best paper presented at the MIMSVAI 2021 workshop, to acknowledge and encourage excellence in research. The awardee will be presented with a certificate at the workshop.</p>\n\n                <h2 dir="ltr">Submission Guidelines</h2>\n\n                <p dir="ltr">Please submit your paper via this EasyChair link:<br />\n                <a href="https://easychair.org/conferences/?conf=mimsvai2021">https://easychair.org/conferences/?conf=mimsvai2021</a></p>\n\n                <p dir="ltr"><strong>All papers need to be anonymized.</strong> Please submit papers with a maximum length of 5 pages (4-page + 1 references) in ACM SIGCHI Master Article template with 2 columns (<a href="https://ubicomp.org/ubicomp2022/cfp/template-information/" target="_blank"></a>). Please contact us (<a href="mailto:ubicomp.mimsvai@gmail.com">ubicomp.mimsvai@gmail.com</a>) if you have any problems when preparing your submissions.</p>\n\n                <p dir="ltr">At least one author of each accepted paper needs to register for the conference and the workshop itself. During the workshop, each paper will be presented briefly by one of the authors. And, the Best Paper Award will be given to the best paper presented at the MIMSVAI 2021 workshop, to acknowledge and encourage excellence in research. The awardee will be presented with a certificate at the workshop.</p>\n\n                <p dir="ltr">The accepted papers will be published in the UbiComp/ISWC Adjunct Proceedings, which will be included in the ACM Digital Library as part of the UbiComp conference supplemental proceedings.</p>\n                '}}},methods:{}},p,!1,function(e){t("2FDy")},"data-v-4e6fe808",null).exports,Staff:t("VU/8")({props:["year"],data:function(){return{name:"ORGANIZERS",data:{2023:[{title:"Workshop Chairs",staff:[{name:"Chuang-Wen You",agency:"National Tsing Hua University, Taiwan",href:"https://nthu-ubicomp-lab.github.io/#/members/cwyou/home"},{name:"Yi-Chao Chen",agency:"Shanghai Jiao Tong University, China",href:"http://www.cs.sjtu.edu.cn/~yichao/pmwiki/pmwiki.php"},{name:"Liwei Chan",agency:"National Yang Ming Chiao Tung University, Taiwan",href:"https://people.cs.nctu.edu.tw/~liweichan/"},{name:"Min-Chun Hu",agency:"National Tsing Hua University, Taiwan",href:"http://mislab.cs.nthu.edu.tw"},{name:"Wei Sun",agency:"Samsung Research America",href:"https://scholar.google.com/citations?user=odTy4-YAAAAJ&hl=zh-CN"},{name:"Yun-Jui Lee",agency:"Corma New Media",href:"http://www.corma.com.tw"}]},{title:"Technical Program Committees",staff:[{name:"Hung-Kuo Chu",agency:"National Tsing Hua University, Taiwan",href:""},{name:"Hao Pan",agency:"Microsoft Research Asia, China",href:""},{name:"Ping-Hsuan Han",agency:"National Taipei University of Technology, Taiwan",href:""},{name:"Xiaoyu Ji",agency:"Zhejiang University, China",href:""},{name:"Lik-Hang Lee",agency:"Hong Kong Polytechnic University, China",href:""},{name:"Rong-Hao Liang",agency:"TU Eindhoven, the Netherlands",href:""},{name:"Feng Lyu",agency:"Central South University, China",href:""},{name:"Takuji Narumi",agency:"University of Tokyo, Japan",href:""},{name:"Tse-Yu Pan",agency:"National Taiwan University of Science and Technology, Taiwan",href:""},{name:"Qian Zhang",agency:"Shanghai Jiao Tong University, China",href:""},{name:"Shih-Wei Sun",agency:"Taipei National University of the Arts, Taiwan",href:""},{name:"Hsin-Ruey Tsai",agency:"National Chengchi University, Taiwan",href:""},{name:"Chiu-Hsuan Wang",agency:"National Yang Ming Chiao Tung University, Taiwan",href:""},{name:"Shigeo Yoshida",agency:"University of Tokyo, Japan",href:""},{name:"Sangki Yun",agency:"Facebook",href:""},{name:"Jinbei Zhang",agency:"Sun Yat-sen University, China",href:""}]},{title:"Web Chair",staff:[{name:"Yan-Ming Chen",agency:"National Tsing Hua University, Taiwan",href:""}]}],2022:[{title:"Workshop Chairs",staff:[{name:"Chuang-Wen You",agency:"National Tsing Hua University, Taiwan",href:"https://nthu-ubicomp-lab.github.io/#/members/cwyou/home"},{name:"Yi-Chao Chen",agency:"Shanghai Jiao Tong University, China",href:"http://www.cs.sjtu.edu.cn/~yichao/pmwiki/pmwiki.php"},{name:"Hsin-Ruey Tsai",agency:"National Chengchi University, Taiwan",href:"https://hsnuhrt.github.io"},{name:"Chu-Yin Chen",agency:"Paris 8 University, French",href:"https://inrev.univ-paris8.fr/chu-yin-chen-pr"}]},{title:"Technical Program Committees",staff:[{name:"Liwei Chan",agency:"National Yang Ming Chiao Tung University, Taiwan",href:""},{name:"Hung-Kuo Chu",agency:"National Tsing Hua University, Taiwan",href:""},{name:"Hao Pan",agency:"Microsoft Research Asia, China",href:""},{name:"Ping-Hsuan Han",agency:"National Taipei University of Technology, Taiwan",href:""},{name:"Min-Chun Hu",agency:"National Tsing Hua University, Taiwan",href:""},{name:"Xiaoyu Ji",agency:"Zhejiang University, China",href:""},{name:"Lik-Hang Lee",agency:"KAIST, South Korea",href:""},{name:"Rong-Hao Liang",agency:"TU Eindhoven, the Netherlands",href:""},{name:"Feng Lyu",agency:"Central South University, China",href:""},{name:"Takuji Narumi",agency:"University of Tokyo, Japan",href:""},{name:"Shih-Wei Sun",agency:"Taipei National University of the Arts, Taiwan",href:""},{name:"Shigeo Yoshida",agency:"University of Tokyo, Japan",href:""},{name:"Sangki Yun",agency:"Facebook",href:""},{name:"Jinbei Zhang",agency:"Sun Yat-sen University, China",href:""}]},{title:"Web Chair",staff:[{name:"Chieh-Jui Ho",agency:"National Tsing Hua University, Taiwan",href:""}]}],2021:[{title:"Workshop Chairs",staff:[{name:"Chuang-Wen You",agency:"National Tsing Hua University, Taiwan",href:"https://nthu-ubicomp-lab.github.io/#/members/cwyou/home"},{name:"Yi-Chao Chen",agency:"Shanghai Jiao Tong University, China",href:"http://www.cs.sjtu.edu.cn/~yichao/pmwiki/pmwiki.php"},{name:"Hsin-Ruey Tsai",agency:"National Chengchi University, Taiwan",href:"https://hsnuhrt.github.io"},{name:"Bin Sheng",agency:"Shanghai Jiao Tong University, China",href:"http://www.cs.sjtu.edu.cn/en/PeopleDetail.aspx?id=149"}]},{title:"Technical Program Committees",staff:[{name:"Andrea Bianchi",agency:"KAIST, South Korea",href:""},{name:"Liwei Chan",agency:"National Yang Ming Chiao Tung University, Taiwan",href:""},{name:"Lung-Pan Cheng",agency:"National Taiwan University, Taiwan",href:""},{name:"Yushi Cheng",agency:"Tsinghua University, China",href:""},{name:"Ping-Hsuan Han",agency:"National Taipei University of Technology, Taiwan",href:""},{name:"Min-Chun Hu",agency:"National Tsing Hua University, Taiwan",href:""},{name:"Xiaoyu Ji",agency:"Zhejiang University, China",href:""},{name:"Lik-Hang Lee",agency:"KAIST, South Korea",href:""},{name:"Rong-Hao Liang",agency:"TU Eindhoven, the Netherlands",href:""},{name:"Feng Lyu",agency:"Central South University, China",href:""},{name:"Koya Narumi",agency:"University of Tokyo, Japan",href:""},{name:"Takuji Narumi",agency:"University of Tokyo, Japan",href:""},{name:"Jun Nishida",agency:"University of Chicago, USA",href:""},{name:"Shih-Wei Sun",agency:"Taipei National University of the Arts, Taiwan",href:""},{name:"Lu Tang",agency:"Xiamen University, China",href:""},{name:"Guangtao Xue",agency:"Shanghai Jiao Tong University, China",href:""},{name:"Shigeo Yoshida",agency:"University of Tokyo, Japan",href:""},{name:"Sangki Yun",agency:"Facebook",href:""},{name:"Jinbei Zhang",agency:"Sun Yat-sen University, China",href:""},{name:"Xinlei Zhang",agency:"University of Tokyo, Japan",href:""}]},{title:"Web Chair",staff:[{name:"Chieh-Jui Ho",agency:"National Tsing Hua University, Taiwan",href:""}]}]}}},methods:{}},d,!1,function(e){t("iogL")},"data-v-4d95c923",null).exports,Agenda:t("VU/8")({props:["year"],data:function(){return{name:"AGENDA",thList:["Time","Event"],discription:{2023:"The program on 10/9, in Cancún (GMT -5) time zone.",2022:"The program on 9/11, in Cambridge (GMT+1) time zone.",2021:"The program on 9/26, in Taiwan (GMT+8) time zone."},rowList:{2023:[{time:"9:00 ~ 9:20",content:"Opening"},{time:"9:20 ~ 10:30",content:"Keynote Talk I",more:" Keynote Speaker: \n • Ashutosh Dhekne (Georgia Institute of Technology, USA)"},{time:"10:30 ~ 10:50",content:"Coffee Break"},{time:"10:50 ~ 12:10",content:"Paper Session",more:" •  “Meta Flow Experience: Exploring group mindfulness meditation in the immersive environment with multi senses feedback”, Jih Hsuan Peng, Ping-Hsuan Han, Tung-Hua Yang, Cheng-Ta Yang, Ze-Song Chen, Yi-Min Huang \n  • “Non-Contact Thermal Haptics for VR”, Yida wangyida, Yi-Chao Chen\n • “Toward an Ever-present Extended Reality: Distinguishing Between Real and Virtual”, Anton Nijholt\n • “Experience: Large Scale Indoor Location-based Service in Libraries”, Ling Ma, Runting Zhang, Xiaohua Shi"},{time:"12:10 ~ 14:00",content:"Lunch Break"},{time:"14:00 ~ 15:10",content:"Keynote Talk II",more:" Keynote Speaker: \n • Pan Hao (Microsoft Research Asia)"},{time:"15:10 ~ 15:30",content:"Coffee Break"},{time:"15:30 ~ 16:50",content:"Panel Discussion with Paper Presenters"},{time:"17:00 ~ 18:10",content:"Keynote Talk III"},{time:"18:10 ~ 18:20",content:"Closing & Best Paper Award Ceremony"}],2022:[{time:"9:00 ~ 9:10",content:"Opening"},{time:"9:10 ~ 10:00",content:"Keynote Talk I",more:" Keynote Speaker:\n • Shao-Yi Chien (National Taiwan University, Taiwan & Ganzin Technology, Inc.)"},{time:"10:00 ~ 10:05",content:"Coffee Break"},{time:"10:05 ~ 11:05",content:" Paper Session",more:" •  “MovableBag: Exploring Asymmetric Interaction for Multi-user Exergame in Extended Reality”, Luis Mendez, Ping-Hsuan Han, Ho Yin Ng \n  • “Improve Immersion in Virtual Reality-Based Basketball Training by Haptic Feedback”, Wan-Lun Tsai, Tse-Yu Pan, Min-Chun Hu\n • “Perceptual Modifications in Augmented Reality: A Short Survey”, Anton Nijholt\n • “InertiaVibe: Low-fidelity Simulation of Inertia using Head-mounted Vibrotactile Feedback to Reduce Cybersickness and Enhance VR Experience”, Shih-Yu Ma, Cong Min Lin, Chung-Wei Wang, Neng-Hao Yu, Mike Y. Chen"},{time:"11:05 ~ 11:10",content:"Coffee Break"},{time:"11:10 ~ 12:00",content:"Keynote Talk II",more:" Keynote Speaker: \n • Mehdi Ammi (University Paris 8, French)"},{time:"12:00 ~ 12:10",content:"Closing & Best Paper Award Ceremony",more:"Best Paper\n •   “MovableBag: Exploring Asymmetric Interaction for Multi-user Exergame in Extended Reality”, Luis Mendez, Ping-Hsuan Han, Ho Yin Ng"}],2021:[{time:"9:00 ~ 9:10",content:"Opening"},{time:"9:10 ~ 10:20",content:"Keynote Talk I",more:" Keynote Speaker:\n • Pedro Lopes (University of Chicago, USA)"},{time:"10:20 ~ 10:30",content:"Coffee Break"},{time:"10:30 ~ 11:10",content:" Paper Session I: Making VR/AR interactions social",more:"  • “CELIP: Ultrasonic-based Lip Reading with Channel Estimation Approach for Virtual Reality Systems”, Yongzhao Zhang, Yi-Chao Chen, Haonan Wang, Xingyu Jin\n • “Experiencing Social Augmented Reality in Public Spaces”, Anton Nijholt"},{time:"11:10 ~ 11:30",content:"Coffee Break"},{time:"11:30 ~ 12:30",content:"Paper Session II: Realistic AR/VR environment matters",more:"  • “ARToken: A Tangible Device for Dynamically Binding Real-world Objects with Virtual Representation”, Hsuan-Yu Hsueh, Chien-Hua Chen, Irene Chen, Chih-Yuan Yao, Hung-Kuo Chu\n • “CravingProbe: A System Combining Virtual Reality and Biofeedback Technologies to Assist Drug Psychotherapy”, Chieh-Jui Ho, Chi-Ting Hou, Min-Wei Hung, Chien Wen (Tina) Yuan, Nanyi Bi, Ming-Chyi Huang, Chuang-Wen You\n • “Material Identification System with Sound Simulation Assisted Method in VR/AR Scenarios”, Yezhou Wang, Runting Zhang, Haonan Wu, Guangtao Xue"},{time:"12:30 ~ 14:00",content:"Lunch Break"},{time:"14:00 ~ 15:10",content:"Keynote Talk II",more:" Keynote Speaker: \n • Mike Chen (National Taiwan University, Taiwan)"},{time:"15:10 ~ 15:20",content:"Coffee Break"},{time:"15:20 ~ 16:00",content:"Paper Session III: Innovative VR/AR Haptic Sensations",more:"  • “High-Speed Non-Contact Thermal Display Using Infrared Rays and Shutter Mechanism”, Sosuke Ichihashi, Arata Horie, Masaharu Hirose, Zendai Kashino, Shigeo Yoshida, Masahiko Inami\n • “Flowing-Haptic Sleeve: Research on Apparent Tactile Motion Applied to Simulating the Feeling of Flow on the Arm”, Hao-Ping Chien, Ming-Cheng Wu, Chun-Cheng Hsu"},{time:"16:00 ~ 16:20",content:"Coffee Break"},{time:"16:20 ~ 17:20",content:"Panel Discussion: The Future Trends in VR/AR Interactions",more:" Panelists: \n • Andrea Bianchi (KAIST)\n • Liwei Chan (National Yang Ming Chiao Tung University)\n • Min-Chun Hu (National Tsing Hua University)\n • Ray Lee (CORMA New Media)"},{time:"17:20 ~ 17:30",content:"Closing & Best Paper Award Ceremony",more:"Best Paper \n •  “High-Speed Non-Contact Thermal Display Using Infrared Rays and Shutter Mechanism”, Sosuke Ichihashi, Arata Horie, Masaharu Hirose, Zendai Kashino, Shigeo Yoshida, Masahiko Inami"}]}}},methods:{}},u,!1,function(e){t("xLSW")},"data-v-0909e336",null).exports,Contact:t("VU/8")({props:["year"],data:function(){return{name:"THE VENUE",body:{2023:'<p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;">The MIMSVAI 2023 workshop is part of </span><a style="color: #e89700;" href="https://www.ubicomp.org/ubicomp2023/">UbiComp 2023↗︎</a>, which will be held in Cancún, Mexico.</p>',2022:'<p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;">The MIMSVAI 2022 workshop is part of </span><a style="color: #e89700;" href="https://www.ubicomp.org/ubicomp2022/">UbiComp 2022↗︎</a>, which will be held virtually.</p>',2021:'<p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;">The MIMSVAI 2021 workshop is part of (co-located with) </span><a href="https://www.ubicomp.org/ubicomp2021/" target="blank" style="color: #e89700;">UbiComp 2021↗︎</a><span>, which will be held virtually.</span></p>'}}},methods:{}},m,!1,function(e){t("b7Ev")},"data-v-01f843f0",null).exports,Keynotes:t("VU/8")({props:["year"],data:function(){return{name:"KEYNOTES",speakers:{2023:[{img:"https://mimsvai.github.io/static/imgs/Ashutosh_Dhekne_headshot.jpg",name:"Ashutosh Dhekne",agency:"Georgia Institute of Technology, USA",keynote:"Keynote 1",content:"Through a UWB Looking Glass: Localization, Orientation, and Gestures",summary:"<p>The ability to accurately measure distances using radio-frequency signals opens up new possibilities for the world of VR, AR, and mixed reality. In this talk we try to address common problems in the VR/AR space via the use of ultra-wideband (UWB) radios, with support from inertial sensors when needed. We first introduce an infinitely scalable UWB-based indoor localization system that allows determining one’s indoor location in a GPS-like privacy preserving fashion. As a fundamental primitive, this allows multiple VR headsets and hand-held controllers to localize themselves in an indoor space. Similarly, real-life objects that can be manipulated in the VR world, can also benefit from self-localization simply using a UWB and potentially an IMU sensor. We then think about facing direction with respect to the torso as a distance measurement problem (with funny looking head-gear). Finally, we address capturing body gestures using wearable sensors that measure inter-appendage distances. In short, this talk is about exploring new ways of obtaining vital information necessary for immersive VR and AR systems, without relying on outward facing cameras. From an accuracy perspective, wireless ranging is not there yet, and cannot compete with visual systems, as yet. Thus, this talk is also an invitation for the research community to look closely at wireless distance measurement primitives and improve the state-of-the-art. Wireless ranging and sensing offers benefits such as ability to operate in dark-rooms, bright featureless rooms, operate from behind cardboard props, operate in fog and other environments, which all promise an immersive experience for VR/AR consumers. Finally, wireless ranging offers a localization solution with negligible compute load.</p>",biography:'<p><a href="https://faculty.cc.gatech.edu/~dhekne/" target="_blank">Dr. Ashutosh Dhekne</a> is an assistant professor at the School of Computer Science at Georgia Tech. His research interests span mobile computing and IoT, and is particularly interested in wireless localization, wireless sensing, and wireless networking. He obtained Ph.D. from the University of Illinois at Urbana Champaign, Master in Technology from IIT Bombay, and Bachelors from the University of Pune. Dr. Dhekne recently received the NSF Career award for advancing the state-of-the-art in UWB research. He received the Richard T. Cheng Endowed Fellowship as a Ph.D. student, and is an NTSE scholar.</p>'},{img:"https://mimsvai.github.io/static/imgs/Pan%20Hao.jpg",name:"Hao Pan",agency:"Microsoft Research Asia",keynote:"Keynote 2",content:"Enhancing Wireless Communication Performance in VR/AR Scenarios Based on Metasurface Technology",summary:"<p>In virtual reality (VR) and augmented reality (AR) applications, wireless communication plays a crucial role in ensuring high-quality, low-latency, and high-bandwidth data transmission. To meet the increasingly stringent performance requirements, this keynote speech will explore the use of metasurface technology to enhance wireless communication performance in VR/AR scenarios.</p><p>First, we will emphasize the importance of wireless communication in VR/AR contexts. Factors such as real-time response, low latency, high bandwidth, and multi-user support are essential for providing an immersive and smooth user experience. Moreover, communication security and stability become particularly important in critical applications such as healthcare, military, and education.</p><p>Next, we will discuss in detail the technical aspects of enhancing VR/AR communication performance based on metasurface technology. These aspects include adaptive beamforming, high-speed data transmission, and stealth communication technology. These techniques will help improve communication quality, speed, security, and stability, ultimately offering a superior virtual experience for users.</p><p>Finally, we will present several application cases of wireless communication performance enhancement in VR/AR scenarios based on metasurface technology and discuss the potential of this technology in future developments. With the continuous maturation and optimization of metasurface technology, we can expect to achieve higher performance and lower cost VR/AR devices, further driving the advancement of this field.</p><p>Through this keynote speech, we aim to provide the audience with an in-depth understanding of the application and development prospects of metasurface technology in wireless communication performance in VR/AR scenarios, stimulate innovative thinking, and jointly promote the development of VR/AR technology.</p>",biography:"<p>Pan Hao joined Microsoft Research Asia (Shanghai) as a Researcher in 2022. Before that, he received his Ph.D. in computer science at the Shanghai Jiao Tong University (SJTU) in 2022, and completed undergraduate studies in the Yingcai Honors College of University of Electronic Science and Technology of China (UESTC) in 2016. His research interests focus on networked systems and span the areas of wireless communication and sensing, human-computer interaction, and computer vision.</p>"}],2022:[{img:"https://mimsvai.github.io/static/imgs/Shao-Yi%20Chien.jpeg",name:"Shao-Yi Chien",agency:"National Taiwan University, Taiwan <br>  <br> Ganzin Technology, Inc.",keynote:"Keynote 1",content:"Eye Tracking, the Core User Interface for the Metaverse",summary:"<p>Metaverse is viewed as the next generation of the Internet and is the next big wave in the IT industry. Virtual Reality (VR) and Augmented Reality (AR) headsets will become the new major edge devices, bringing users to immerse into the Internet or fusing the digital and the physical worlds. In this talk, the founder and CEO of Ganzin Technology, Dr. Shao-Yi Chien, will present his point of view about why eye tracking will become the core user interface for AR/VR devices, and the immersive user experience will be achieved by integrating eye tracking with other user interaction subsystems as a multi-modality user interface. Dr. Chien will also show the achievements of Ganzin Technology---the most easy-to-integrate eye tracking solution on the market.</p>",biography:'<p><a href="http://www.ee.ntu.edu.tw/profile1.php?id=101" target="_blank">Shao-Yi Chien</a> received the Ph.D. degree from the Department of Electrical Engineering, National Taiwan University (NTU), Taipei, Taiwan, in 2003. In 2004, he joined the Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, as an Assistant Professor. Since 2012, he has been a Professor. Prof. Chien served as the Chair of IEEE Circuits and Systems Society Multimedia Systems and Applications Technical Committee in 2017&mdash;2019. Dr. Chien is an expert in AR/VR, eye tracking, computer vision, real-time image/video processing, video coding, computer graphics, and system-on-a-chip design. He has published more than 300 papers and granted more than 40 patents. Since 2018, he is the founder and CEO of Ganzin Technology, Inc., an eye tracking solution provider for AR/VR/smart-glasses, which is the most easy-to-integrate solution on the market.</p>'},{img:"http://www.mehdi-ammi.eu/images/mehdi.jpg",name:"Mehdi Ammi",agency:"University Paris 8, French",keynote:"Keynote 2",content:"Social haptic communication and interaction in Metaverse",summary:"<p>The presentation deals with a series of research that address both the expression and perception of emotions with the haptic channel for different type of interpersonal interaction and communication. Our early research explored how people express physically emotions when communicating with 3D avatars. Then, we investigated how the haptic feedback supplements the facial expression of 3D avatars to improve their emotional expressivity and to remove the ambiguity of perception of some close emotions. In this context, we used new methods based the “Information Integration Theory” to study and model the way in which people combine haptic and visual information to perceive emotions. With the same methodology, we investigated other interpersonal communication configurations, in particular, Human-Robot Social Interaction. The objective was to improve the social capacities of robot in daily life (handshake, etc.).</p>",biography:'<p><a href="http://www.mehdi-ammi.eu" target="_blank">Mehdi Ammi</a> is Professor at the University of Paris 8. He is engineer in electronics (2000), PhD in robotics (2005) and habilitaded to supervise research (HDR) in computer science since 2013. His research focuses on the study of VR-AR and pervasive environments while considering their societal and industrial applications (e-health, factory of the future, etc.). He addresses this field with a multidisciplinary approach combining artificial intelligence (machine learning, neurosciences, etc.), mechatronic technologies (organic electronics, smart textiles, etc.) and human factors (psychology, behavioral analysis, etc.). Mehdi Ammi was the leader or coleader of several international working groups (IEEE TCH, EuroVR Haptic SIG, etc.), and was involved in more than 20 national &amp; international projects and industrial collaborations.</p><p>For more info: see<a href="https://mikechen.com/" target="_blank"> </a><a href="http://www.mehdi-ammi.eu">http://www.mehdi-ammi.eu</a></p>'}],2021:[{img:"http://plopes.org/wp-content/uploads/2019/05/PedroLopes_UChicago2.jpg",name:"Pedro Lopes",agency:"University of Chicago, USA",keynote:"Keynote 1",content:"Human Computer Integration: Powering a New Generation of Interactive Devices",summary:'<p><span style="font-weight: 400;">When we look back to the early days of computing, user and device were distant, often located in separate rooms. Then, in the &rsquo;70s, personal computers &ldquo;moved in&rdquo; with users. In the &rsquo;90s, mobile devices moved computing into users&rsquo; pockets. More recently, wearable devices brought computing into constant physical contact with the user&rsquo;s skin. These transitions proved useful: moving closer to users allowed interactive devices to sense more of their user and act more personal. The main question that drives my research is: </span><strong>what is the next interface paradigm that supersedes wearable devices?</strong></p><p><span style="font-weight: 400;">The primary way researchers have been investigating this is by asking where future interactive devices will be located with respect to the user&rsquo;s body. Many posit that the next generation of interfaces will be implanted inside the user&rsquo;s body. However, I argue that their location with respect to the user&rsquo;s body is not the primary factor; in fact, implanted devices are already happening in that we have pacemakers, insulin pumps, etc. Instead, I argue that the key factor is how will devices </span><strong>integrate</strong><span style="font-weight: 400;"> with the user&rsquo;s biological senses and actuators.&nbsp;</span></p><p><span style="font-weight: 400;">This body-device integration allows us to engineer interactive devices that intentionally borrow parts of the body for input and output, rather than adding more technology to the body. For example, one such type of body-integrated devices, which I have advanced recently, are interactive systems based on electrical muscle stimulation. These devices are able to move their user&rsquo;s muscles using computer-controlled electrical impulses, achieving the functionality of robotic exoskeletons without the bulky motors.&nbsp;</span></p><p><span style="font-weight: 400;">They key insight of my research is that engineering devices that intentionally borrow parts of the user&rsquo;s biology puts forward a </span><strong>new</strong> <strong>generation of miniaturized devices</strong><span style="font-weight: 400;">; allowing us to circumvent traditional physical constrains. For instance, in the case of our devices based on electrical muscle stimulation, they demonstrate how our body-device integration circumvents the constrains imposed by ratio of electrical power and size of a motor (i.e., the stronger/larger a motor is, more current needed to actuate it). Similarly, we demonstrate how our body-device integration approach </span><strong>even allowed us to miniaturize thermal feedback</strong><span style="font-weight: 400;"> (hot/cold sensations) without the need for power-hungry devices like Peltiers, air conditioners or heaters.</span></p>',biography:'<p><a href="https://lab.plopes.org/"><span style="font-weight: 400;color: rgb(17, 85, 204);text-decoration: underline;">Pedro Lopes</span></a><span style="font-weight: 400;"> is an Assistant Professor in Computer Science at the University of Chicago, where he leads the </span><strong>Human Computer Integration lab</strong><span style="font-weight: 400;">. Pedro focuses on </span><strong>integrating computer interfaces with the human body&mdash;exploring the interface paradigm that supersedes wearable computing</strong><span style="font-weight: 400;">. Some of these new integrated-devices include: a device based on muscle stimulation that allows </span><a href="https://www.youtube.com/watch?v=Gz4dphzBb6I"><span style="font-weight: 400;color: rgb(17, 85, 204);    text-decoration: underline;">users to manipulate tools they never seen before</span></a><span style="font-weight: 400;"> or that </span><a href="https://www.youtube.com/watch?v=1BT8REEJibM"><span style="font-weight: 400;color: rgb(17, 85, 204);text-decoration: underline;">accelerate their reaction time</span></a><span style="font-weight: 400;">, or a device that </span><a href="https://www.youtube.com/watch?v=pH68GNkb_fA&amp;feature=youtu.be"><span style="font-weight: 400;color: rgb(17, 85, 204); text-decoration: underline;">leverages the sense of smell to create an illusion of temperature</span></a><span style="font-weight: 400;">. </span><span style="font-weight: 400;">Pedro&rsquo;s work is published at top-tier conferences (ACM CHI &amp; ACM UIST), where he has received four Best Paper awards, two Best Paper nominations and several Best Talk/Demo/Video awards. Pedro&rsquo;s work also captured the interest of media, such as New York Times, MIT Technology Review, NBC, Discovery Channel, NewScientist, Wired and has been shown at Ars Electronica and World Economic Forum (More: </span><a href="https://lab.plopes.org/"><span style="font-weight: 400;color: rgb(17, 85, 204);text-decoration: underline;">https://lab.plopes.org</span></a><span style="font-weight: 400;">)</span></p>'},{img:"https://pbs.twimg.com/profile_images/502132318354419713/UXyxLGjS_400x400.jpeg",name:"Mike Chen",agency:"National Taiwan University, Taiwan",keynote:"Keynote 2",content:"Playing with Perception - Haptics Re-imagined",summary:'<p><span style="font-weight: 400;">Motion simulator was invented over 100 years ago and modern haptic feedback devices were commercialized over 50 years ago. In this talk, we will take a fresh look at the fundamental limitations that have been in place ever since, and re-imaging how these experiences can be perceptually designed to be more realistic, immersive, and enable greater mobility.</span><span style="text-decoration: underline;"><span style="color: #3366ff;"><a style="color: #3366ff;" href="https://dl.acm.org/doi/10.1145/3386569.3392482"> <span style="font-weight: 400;">HeadBlaster</span></a></span></span><span style="font-weight: 400;">, is the first wearable approach to recreating the persistent proprioception and vestibular stimulation during real acceleration, enabling it to provide significantly longer motion sensation than motion platforms.</span><span style="text-decoration: underline;"><span style="color: #3366ff;"><a style="color: #3366ff; text-decoration: underline;" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376292"> MiniatureHaptics</a></span></span><span style="font-weight: 400;"> explores whether haptic feedback can be remapped to just the hand, in order to enable new haptic experiences that are not practical to create at the full, human-body scale.&nbsp;&nbsp;</span></p><p><span style="font-weight: 400;">We will further explore research opportunities in applying perceptual design to haptic feedback, such as a multi-sensory approach that utilizes noise, which is normally undesir</span><span style="font-weight: 400;">able, to enhance haptic experiences, as well as perceptual approaches to address limitations of existing force feedback technologies. Last, we will discuss fun research and design opportunities by shifting focus from reproducing real-world experiences to out-of-this-world experiences. </span></p>',biography:'<p><a href="https://mikechen.com/"><span style="font-weight: 400;"><span style="text-decoration: underline;"><span style="color: #3366ff; text-decoration: underline;">Prof. Mike Y. Chen</span></span></span></a><span style="font-weight: 400;"> creates future user experiences, and innovates at the intersection of HCI, AI, design, perception, and VR/AR. He founded the</span><span style="text-decoration: underline;"><span style="color: #3366ff;"><a style="color: #3366ff;" href="https://ntuhci.org/"> <span style="font-weight: 400;">HCI Lab</span></a></span></span><span style="font-weight: 400;"> at National Taiwan University, which has published at premier ACM conferences including CHI, SIGGRAPH, and UIST, and received Best Talk/Paper and Innovative Game Design Awards. He has</span><span style="text-decoration: underline;"><span style="color: #3366ff; text-decoration: underline;"><a style="color: #3366ff; text-decoration: underline;" href="https://github.com/ntu-hci-lab"> <span style="font-weight: 400;">open-sourced projects</span></a></span></span><span style="font-weight: 400;"> ranging from hardware designs of haptic systems to touchscreen gesture data analytics for ResearchKit, which was mentioned at Apple WWDC 2019.</span></p><p><span style="font-weight: 400;">Mike received his Ph.D. in Computer Science from UC Berkeley, with a Management of Technology (MOT) certificate from Haas School of Business. Prior to NTU, Mike led Mobile R&amp;D at Intel Research and startup Ludic Labs.&nbsp;</span></p><p><span style="font-weight: 400;">For more info: see</span><span style="text-decoration: underline;"><span style="color: #3366ff;"><a style="color: #3366ff;" href="https://mikechen.com/"> <span style="font-weight: 400;">https://mikechen.com/</span></a></span></span></p>'}]}}},methods:{}},g,!1,function(e){t("T0Da")},"data-v-13bca3cc",null).exports,Panel:t("VU/8")({props:["year"],data:function(){return{name:"PANEL DISCUSSION",panels:{2022:{texts:["Panel Discussion: The Future Trends in VR/AR Interactions","Panelists:"],lis:[]},2021:{texts:["Panel Discussion: The Future Trends in VR/AR Interactions","Panelists:"],lis:["Andrea Bianchi (KAIST)","Liwei Chan (National Yang Ming Chiao Tung University)","Min-Chun Hu (National Tsing Hua University)","Ray Lee (CORMA New Media)"]}}}},methods:{}},f,!1,function(e){t("Y6Fo")},"data-v-854a1336",null).exports},data:function(){return{data:"",menuList:"",fbImg:"https://mimsvai.github.io/static/imgs/facebook-loge.png",twitterImg:"https://mimsvai.github.io/static/imgs/twitter-logo.png",fbUrl:" https://www.facebook.com/mimsvai",twUrl:"https://twitter.com/mimsvai",isTop:!1,year:2023}},mounted:function(){window.addEventListener("scroll",this.handleScroll)},created:function(){this.$route.query.year?this.year=this.$route.query.year:this.year=2023},watch:{$route:function(e,n){window.scrollTo(0,0),this.$route.query.year?this.year=this.$route.query.year:this.year=2023}},methods:{handleScroll:function(e,n){window.scrollY>300?this.isTop=!1:this.isTop=!0}}},w={render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{attrs:{id:"app"}},[t("MenuM",{attrs:{year:e.year}}),t("section",{attrs:{id:"top","data-aos":"fade-down","data-aos-duration":"2000"}},[t("Intro",{attrs:{year:e.year}})],1),t("section",{staticClass:"About section",attrs:{id:"ABOUT","data-aos":"fade-up","data-aos-duration":"1000"}},[t("About",{attrs:{year:e.year}})],1),t("section",{staticClass:"Paper section",attrs:{id:"PAPERS"}},[t("Paper",{attrs:{year:e.year}})],1),t("section",{staticClass:"Keynotes section",attrs:{id:"KEYNOTES"}},[t("Keynotes",{attrs:{year:e.year}})],1),2021==e.year?t("section",{staticClass:"Panel section",attrs:{id:"PANEL"}},[t("Panel",{attrs:{year:e.year}})],1):e._e(),t("section",{staticClass:"Agenda section",attrs:{id:"AGENDA"}},[t("Agenda",{attrs:{year:e.year}})],1),t("section",{staticClass:"Organizera section",attrs:{id:"ORGANIZER"}},[t("Staff",{attrs:{year:e.year}})],1),t("section",{staticClass:"Venue section",attrs:{id:"VENUE"}},[t("Contact",{attrs:{year:e.year}})],1),e.year>"2021"?t("section",{staticClass:"Past section"},[t("div",{staticClass:"contentContainer"},[t("h1",{staticClass:"TitleU"},[e._v("Past Workshop")]),e.year>"2022"?t("router-link",{staticClass:"textSize",attrs:{to:"/?year=2022"}},[e._v("MIMSVAI 2022, Virtual, Global"),t("br")]):e._e(),t("router-link",{staticClass:"textSize",attrs:{to:"/?year=2021"}},[e._v("MIMSVAI 2021, Virtual, Global")])],1)]):e._e(),t("div",{staticClass:"socialCantainer",class:{"social-top":e.isTop}},[t("a",{staticClass:"socialIcon fbIcon",style:{"background-image":"url("+e.fbImg+")"},attrs:{href:e.fbUrl,target:"_blank"}}),t("a",{staticClass:"socialIcon twIcon",style:{"background-image":"url("+e.twitterImg+")"},attrs:{href:e.twUrl,target:"_blank"}})])],1)},staticRenderFns:[]};var v=t("VU/8")(y,w,!1,function(e){t("h74h")},null,null).exports;i.a.use(o.a);var b=new o.a({mode:"history",base:Object({NODE_ENV:"production"}).BASE_URL,routes:[{path:"/",name:"HomePage",component:v}]}),C=t("RInU"),T=t.n(C);t("AhPL");i.a.config.productionTip=!1,new i.a({created:function(){T.a.init()},el:"#app",router:b,components:{App:s},template:"<App/>"})},T0Da:function(e,n){},Y6Fo:function(e,n){},b7Ev:function(e,n){},dXnZ:function(e,n){},h74h:function(e,n){},iogL:function(e,n){},t5nL:function(e,n){},tXKz:function(e,n){},xLSW:function(e,n){}},["NHnr"]);
//# sourceMappingURL=app.83712fe6bf4508fe9a51.js.map