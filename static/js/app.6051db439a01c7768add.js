webpackJsonp([0],{"1k2W":function(e,t){},"2DBd":function(e,t){},"3x5y":function(e,t){},"A/i9":function(e,t){},AhPL:function(e,t){},Ji7Z:function(e,t){},NHnr:function(e,t,n){"use strict";Object.defineProperty(t,"__esModule",{value:!0});var a=n("7+uW"),i={render:function(){var e=this.$createElement,t=this._self._c||e;return t("div",{attrs:{id:"app"}},[t("transition",{attrs:{name:"page",mode:"out-in"}},[t("router-view")],1)],1)},staticRenderFns:[]};var o=n("VU/8")({name:"App",components:{}},i,!1,function(e){n("t5nL")},null,null).exports,s=n("/ocq"),r={data:function(){return{iconSrc:"https://caepo.org/images/menu-logo.svg",menuList:[{name:"ABOUT",id:"ABOUT"},{name:"CALL FOR PAPERS",id:"PAPERS"},{name:"KEYNOTES",id:"KEYNOTES"},{name:"PANEL DISCUSSION",id:"PANEL"},{name:"AGENDA",id:"AGENDA"},{name:"ORGANIZER",id:"ORGANIZER"},{name:"VENUE",id:"VENUE"}],drawerStatus:!1,menuclassList:{menuIconContainer:!0,change:!1},name:"Oppps",isTop:!0,totopImg:"https://mimsvai.github.io/static/imgs/toTopIcon.png"}},watch:{$route:function(){this.name=this.$route.params.PID}},mounted:function(){this.name=this.$route.params.PID,window.addEventListener("scroll",this.handleScroll)},methods:{opentheDrawer:function(){this.drawerStatus=!this.drawerStatus,this.menuclassList.change=!this.menuclassList.change},handleScroll:function(e,t){window.scrollY>100?this.isTop=!1:this.isTop=!0},scroll_to:function(e){var t=document.getElementById(e).offsetTop;window.scrollTo(0,t)}}},l={render:function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{attrs:{id:"menuContainer"}},[n("a",{class:{"lego-top":e.isTop},attrs:{id:"Lego",href:"#"}},[n("h1",[e._v("MIMSVAI 2021")])]),n("h1",{staticClass:"TitleB"},[e._v(e._s(e.name))]),e._l(e.menuList,function(t){return n("div",{staticClass:"menuItem",class:{"menu-top":e.isTop},on:{click:function(n){return e.scroll_to(t.id)}}},[e._v(e._s(t.name)),n("div",{attrs:{id:"underline"}})])}),e.drawerStatus?n("div",{attrs:{id:"drawer"}},e._l(e.menuList,function(t,a){return n("div",{staticClass:"drawerItem",on:{click:function(t){return e.opentheDrawer()}}},[n("a",{attrs:{href:"#"+t.name}},[e._v(e._s(t.name)),n("div",{attrs:{id:"underline"}})])])}),0):e._e(),n("div",{staticClass:"menuIcon",class:e.menuclassList,on:{click:function(t){return e.opentheDrawer()}}},[n("div",{staticClass:"menuIconBar1"}),n("div",{staticClass:"menuIconBar2"}),n("div",{staticClass:"menuIconBar3"})]),n("a",{directives:[{name:"show",rawName:"v-show",value:!e.isTop,expression:"!isTop"}],staticClass:"toTopIcon",style:{"background-image":"url("+e.totopImg+")"},attrs:{href:"#"}})],2)},staticRenderFns:[]};var p={render:function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"introContainer"},[n("div",{staticClass:"introImg",style:{"background-image":"url("+e.src+")"}},[n("div",{staticClass:"introMask"},[n("div",{staticClass:"introText"},[e._v(e._s(e.text1))]),n("h1",{staticClass:"TitleU"},[e._v(e._s(e.name))]),n("div",{staticClass:"introText",domProps:{innerHTML:e._s(e.text2)}})])])])},staticRenderFns:[]};var c={render:function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"contentContainer"},[t("h1",{staticClass:"TitleU"},[this._v(this._s(this.name))]),t("div",{staticClass:"HTMLContainer",domProps:{innerHTML:this._s(this.body)}})])},staticRenderFns:[]};var h={render:function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"contentContainer"},[t("h1",{staticClass:"TitleU"},[this._v(this._s(this.name))]),t("div",{staticClass:"HTMLContainer",domProps:{innerHTML:this._s(this.body)}})])},staticRenderFns:[]};var g={render:function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"sraffPage"},[n("h1",{staticClass:"TitleU"},[e._v(e._s(e.name))]),e._l(e.data,function(t){return n("div",{staticClass:"staffContainer"},[n("h1",{staticClass:"chairTitle"},[e._v(e._s(t.title))]),n("div",{staticClass:"staffListContainer"},e._l(t.staff,function(t){return n("div",{staticClass:"staffListContentBlock",attrs:{"data-aos":"fade-up","data-aos-duration":"2000"}},[""!=t.href?n("a",{staticClass:"staffListName",attrs:{href:t.href,target:"_blank"}},[e._v(e._s(t.name))]):e._e(),""==t.href?n("div",{staticClass:"staffListName"},[e._v(e._s(t.name))]):e._e(),n("div",{staticClass:"staffListTitle"},[e._v(e._s(t.agency))])])}),0)])})],2)},staticRenderFns:[]};var d={render:function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"contentContainer"},[n("h1",{staticClass:"TitleU"},[e._v(e._s(e.name))]),n("p",[e._v("The program on 9/26, in Taiwan (GMT+8) time zone.")]),n("table",[n("thead"),n("tr",{staticClass:"headRow"},e._l(e.thList,function(t,a){return n("th",{key:a,staticClass:"headLabel"},[e._v(e._s(t))])}),0),n("tbody"),e._l(e.rowList,function(t,a){return n("tr",{key:a},[n("td",[n("strong",[e._v(e._s(t.time))])]),n("td",{staticStyle:{"white-space":"pre-wrap"}},[n("strong",[e._v(e._s(t.content))]),t.more?n("p",{staticClass:"tableCotent"},[e._v(e._s(t.more))]):e._e()])])})],2)])},staticRenderFns:[]};var f={render:function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"contentContainer"},[t("h1",{staticClass:"TitleU"},[this._v(this._s(this.name))]),t("div",{staticClass:"HTMLContainer",domProps:{innerHTML:this._s(this.body)}})])},staticRenderFns:[]};var u={render:function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"contentContainer"},[n("h1",{staticClass:"TitleU"},[e._v(e._s(e.name))]),n("h2",{staticClass:"subTitle"},[e._v("Keynote Speakers")]),e._l(e.speakers,function(t){return n("div",{staticClass:"speakerContainer",attrs:{"data-aos":"fade-up","data-aos-duration":"1000"}},[n("div",{staticClass:"speakerImg",style:{"background-image":"url("+t.img+")"}}),n("div",{staticClass:"staffListContentBlock"},[n("h2",{staticClass:"staffListName"},[e._v(e._s(t.name))]),n("div",{staticClass:"staffListTitle"},[e._v(e._s(t.agency))]),n("h3",[e._v(e._s(t.keynote)+" :")]),n("p",[e._v(e._s(t.content))]),""!=t.summary?n("h3",[e._v("Summary :")]):e._e(),n("div",{staticClass:"HTMLContainer",domProps:{innerHTML:e._s(t.summary)}}),""!=t.biography?n("h3",[e._v("Biography :")]):e._e(),n("div",{staticClass:"HTMLContainer",domProps:{innerHTML:e._s(t.biography)}})])])})],2)},staticRenderFns:[]};var m={render:function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"contentContainer"},[t("h1",{staticClass:"TitleU"},[this._v(this._s(this.name))]),this._m(0)])},staticRenderFns:[function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"textSize",attrs:{"data-aos":"fade-up"}},[t("p",{staticClass:"textSize"},[this._v("Panel Discussion: The Future Trends in VR/AR Interactions")]),t("p",{staticClass:"textSize"},[this._v("Panelists: ")]),t("ul",[t("li",{staticClass:"textSize"},[this._v("Andrea Bianchi (KAIST)")]),t("li",{staticClass:"textSize"},[this._v("Liwei Chan (National Yang Ming Chiao Tung University)")]),t("li",{staticClass:"textSize"},[this._v("Min-Chun Hu (National Tsing Hua University)")]),t("li",{staticClass:"textSize"},[this._v("Ray Lee (CORMA New Media)")])])])}]};var y={name:"HomePage",components:{MenuM:n("VU/8")(r,l,!1,function(e){n("den4")},null,null).exports,Intro:n("VU/8")({components:{},data:function(){return{name:"The First Workshop on Multiple Input Modalities and Sensations for VR/AR Interactions (MIMSVAI 2021)",src:"https://mimsvai.github.io/static/imgs/banner.jpg",text1:"September 26, 2021, organised virtually",text2:'<span>Workshop in conjunction with </span><a href="https://www.ubicomp.org/ubicomp2021/" target="blank" style="color: #e89700;">UbiComp 2021↗︎</a>'}},methods:{}},p,!1,function(e){n("A/i9")},"data-v-32bf305e",null).exports,About:n("VU/8")({components:{},data:function(){return{name:"ABOUT THE WORKSHOP",body:'<p dir="ltr" style="line-height: 1.7; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 19px; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">In order to gather research momentum from the UbiComp community, we call for papers discussing how to design or incorporate different input modalities or sensations for enabling a more immersive VR/AR experience. We arrange sessions for each accepted paper to be presented in the workshop. At the end of each session, we will assign session chairs to mediate the discussion to distill motivated ideas and findings to the participants.&nbsp;</span></p><br><p dir="ltr" style="line-height: 1.7; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 19px; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Besides the regular sessions, we will invite two experts for VR/AR interactions to give talks in the morning and the afternoon, respectively. We hope these two keynote presentations can bring in more interesting perspectives regarding VR/AR-related researches. Finally, a panel discussion will be held in the late afternoon. We will invite 3-5 panelists from both academia and industrial sides to share their valuable experiences related to current research or development trends of VR/AR applications. </span></p>'}},methods:{}},c,!1,function(e){n("RcHa")},"data-v-53bdd619",null).exports,Paper:n("VU/8")({components:{},data:function(){return{name:"CALL FOR PAPERS",body:'<p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;  color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">With the advance of VR/AR technology, more and more VR/AR&nbsp;</span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">applications are emerging and have been popular among new users.&nbsp;</span></span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">To facilitate user interaction with the systems, multi-modal input&nbsp;</span></span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">modalities are required. Furthermore, coherent integration of&nbsp;</span></span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">multiple realistic sensations can increase the level of realism, and&nbsp;</span></span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">therefore creates a truly immersive VR/AR experience. However,&nbsp;</span></span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">a lack of robust and intuitive interaction interfaces and realistic&nbsp;</span></span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">sensations hinders users&rsquo; experience for achieving a fascinating&nbsp;</span></span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">acceptance in various application areas of VR/AR interactions. This&nbsp;</span></span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">workshop discusses the challenges and applications of designing a&nbsp;</span></span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">higher coherence between the different input modalities and sensations&nbsp;</span></span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">to provide more engaging VR/AR experiences by leveraging&nbsp;</span></span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">the advances of both VR/AR software and hardware to improve </span></span><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">user experience.</span></span></p><h1 dir="ltr" style="margin-top: 18pt; margin-bottom: 6pt; line-height: 1.7;"><span style="font-size: 30px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Topics</span></span></h1><p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"></p><p><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Papers may include, but not be limited to, topics such as:</span></span></p><ul>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">2D/3D and volumetric display and projection technology</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Immersive analytics and visualization</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Modeling and simulation</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Multimodal capturing and reconstruction</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Scene description and management issues</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Storytelling</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Tracking and sensing</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Audio interfaces, sound rendering, spatialized audio, auditory perception and psychoacoustics</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Embodied agents, virtual humans and (self-)avatars</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Haptic and tactile interfaces, wearable haptics, passive haptics, pseudo haptics, other touch-based UI</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Mediated and diminished reality</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Multimodal input and output</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Multisensory rendering, registration, and synchronization</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Perception and cognition</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Presence, body ownership, and agency</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Teleoperation and telepresence</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">3D user interaction</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">3D UI metaphors</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Collaborative interactions</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Ethical issues</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Human factors and ergonomics</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Input devices</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Locomotion and navigation</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Multimodal/cross-modal interaction and perception</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Non-fatiguing 3D UIs</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Non-visual interfaces (such as olfactory)</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Touch, tangible and gesture interfaces</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Usage research, evaluation methods and empirical studies</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Domain-specific VR/AR applications.</span></span></li>    <li><span style="line-height: 1.7;font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Design process, platform/tools, or libraries used to incorporate input modalities or sensations for VR/AR interactions or studying their use</span></span></li></ul><h1 dir="ltr" style="margin-top: 18pt; margin-bottom: 6pt; line-height: 1.7;"><span style="font-size: 30px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Important Dates</span></span></h1><p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Paper submission: 23:59 (AoE), </span><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: line-through; vertical-align: baseline; white-space: pre-wrap;">June 15, 2021</span>&nbsp<span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: line-through; vertical-align: baseline; white-space: pre-wrap;">June 22, 2021 </span><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"> June 30, 2021 </span><span style="font-weight: 400; color: #ff0000;">(extended and final)</span></span></p><p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Paper notification: 23:59 (AoE), July 15, 2021</span></span></p><p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Camera-ready: 23:59 (AoE), <span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: line-through; vertical-align: baseline; white-space: pre-wrap;">July 31, 2021  Aug. 6, 2021 </span><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Aug. 10, 2021 </span><span style="font-weight: 400; color: #ff0000;">(extended)</span></span></span></p><p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Workshop: September 26, 2021</span></span></p><h1 dir="ltr" style="margin-top: 18pt; margin-bottom: 6pt; line-height: 1.7;"><span style="font-size: 30px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Best Paper Award</span></span></h1><p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">The Best Paper Award is given to the best paper presented at the MIMSVAI 2021 workshop, to acknowledge and encourage excellence in research. The awardee will be presented with a certificate at the workshop.</span></span></p><h2 dir="ltr" style="margin-top: 18pt; margin-bottom: 6pt; line-height: 1.7;"><span style="font-size: 30px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Submission Guidelines</span></span></h2><p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">Please submit your paper via this EasyChair link: </span><br><a href="https://easychair.org/conferences/?conf=mimsvai2021" style="text-decoration:none;"><span style=" color: rgb(17, 85, 204); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">https://easychair.org/conferences/?conf=mimsvai2021</span></a></span></p><br><p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><strong  style="color: red;">All papers need to be anonymized.</strong> Please submit papers with a maximum length of 5 pages (4-page + 1 references) in </spsn><a href="https://ubicomp.org/ubicomp2021/cfp/template-information/"><span style=" color: rgb(17, 85, 204); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">ACM SIGCHI Master Article template with 2 columns</span></a><span style="font-size: 19px;"><span style=" color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">. Please contact us (<a href="mailto:ubicomp.mimsvai@gmail.com"><span style=" color: rgb(17, 85, 204); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;">ubicomp.mimsvai@gmail.com</span></a>) if you have any problems when preparing your submissions.</span></span></p><br><p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;  color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">At least one author of each accepted paper needs to register for the conference and the workshop itself. During the workshop, each paper will be presented briefly by one of the authors. And, the Best Paper Award will be given to the best paper presented at the MIMSVAI 2021 workshop, to acknowledge and encourage excellence in research. The awardee will be presented with a certificate at the workshop.</span></p><br><p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;  color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">The accepted papers will be published in the UbiComp/ISWC Adjunct Proceedings, which will be included in the ACM Digital Library as part of the UbiComp conference supplemental proceedings.</span></p>'}},methods:{}},h,!1,function(e){n("Ji7Z")},"data-v-1a4847c9",null).exports,Staff:n("VU/8")({components:{},data:function(){return{name:"ORGANIZERS",data:[{title:"Workshop Chairs",staff:[{name:"Chuang-Wen You",agency:"National Tsing Hua University, Taiwan",href:"https://sites.google.com/site/cwyou2004/"},{name:"Yi-Chao Chen",agency:"Shanghai Jiao Tong University, China",href:"http://www.cs.sjtu.edu.cn/~yichao/pmwiki/pmwiki.php"},{name:"Hsin-Ruey Tsai",agency:"National Chengchi University, Taiwan",href:"https://hsnuhrt.github.io"},{name:"Bin Sheng",agency:"Shanghai Jiao Tong University, China",href:"http://www.cs.sjtu.edu.cn/en/PeopleDetail.aspx?id=149"}]},{title:"Technical Program Committees",staff:[{name:"Andrea Bianchi",agency:"KAIST, South Korea",href:""},{name:"Liwei Chan",agency:"National Yang Ming Chiao Tung University, Taiwan",href:""},{name:"Lung-Pan Cheng",agency:"National Taiwan University, Taiwan",href:""},{name:"Yushi Cheng",agency:"Tsinghua University, China",href:""},{name:"Ping-Hsuan Han",agency:"National Taipei University of Technology, Taiwan",href:""},{name:"Min-Chun Hu",agency:"National Tsing Hua University, Taiwan",href:""},{name:"Xiaoyu Ji",agency:"Zhejiang University, China",href:""},{name:"Lik-Hang Lee",agency:"KAIST, South Korea",href:""},{name:"Rong-Hao Liang",agency:"TU Eindhoven, the Netherlands",href:""},{name:"Feng Lyu",agency:"Central South University, China",href:""},{name:"Koya Narumi",agency:"University of Tokyo, Japan",href:""},{name:"Takuji Narumi",agency:"University of Tokyo, Japan",href:""},{name:"Jun Nishida",agency:"University of Chicago, USA",href:""},{name:"Shih-Wei Sun",agency:"Taipei National University of the Arts, Taiwan",href:""},{name:"Lu Tang",agency:"Xiamen University, China",href:""},{name:"Guangtao Xue",agency:"Shanghai Jiao Tong University, China",href:""},{name:"Shigeo Yoshida",agency:"University of Tokyo, Japan",href:""},{name:"Sangki Yun",agency:"Facebook",href:""},{name:"Jinbei Zhang",agency:"Sun Yat-sen University, China",href:""},{name:"Xinlei Zhang",agency:"University of Tokyo, Japan",href:""}]},{title:"Web Chair",staff:[{name:"Chieh-Jui Ho",agency:"National Tsing Hua University, Taiwan",href:""}]}]}},methods:{}},g,!1,function(e){n("3x5y")},"data-v-7ca39168",null).exports,Agenda:n("VU/8")({components:{},data:function(){return{name:"AGENDA",thList:["Time","Event"],rowList:[{time:"9:00 ~ 9:10",content:"Opening"},{time:"9:10 ~ 10:20",content:"Keynote Talk I",more:" Keynote Speaker:\n • Pedro Lopes (University of Chicago, USA)"},{time:"10:20 ~ 10:30",content:"Coffee Break"},{time:"10:30 ~ 11:10",content:" Paper Session I: Making VR/AR interactions social",more:"  • “CELIP: Ultrasonic-based Lip Reading with Channel Estimation Approach for Virtual Reality Systems”, Yongzhao Zhang, Yi-Chao Chen, Haonan Wang, Xingyu Jin\n • “Experiencing Social Augmented Reality in Public Spaces”, Anton Nijholt"},{time:"11:10 ~ 11:30",content:"Coffee Break"},{time:"11:30 ~ 12:30",content:"Paper Session II: Realistic AR/VR environment matters",more:"  • “ARToken: A Tangible Device for Dynamically Binding Real-world Objects with Virtual Representation”, Hsuan-Yu Hsueh, Chien-Hua Chen, Irene Chen, Chih-Yuan Yao, Hung-Kuo Chu\n • “CravingProbe: A System Combining Virtual Reality and Biofeedback Technologies to Assist Drug Psychotherapy”, Chieh-Jui Ho, Chi-Ting Hou, Min-Wei Hung, Chien Wen (Tina) Yuan, Nanyi Bi, Ming-Chyi Huang, Chuang-Wen You\n • “Material Identification System with Sound Simulation Assisted Method in VR/AR Scenarios”, Yezhou Wang, Runting Zhang, Haonan Wu, Guangtao Xue"},{time:"12:30 ~ 14:00",content:"Lunch Break"},{time:"14:00 ~ 15:10",content:"Keynote Talk II",more:" Keynote Speaker: \n • Mike Chen (National Taiwan University, Taiwan)"},{time:"15:10 ~ 15:20",content:"Coffee Break"},{time:"15:20 ~ 16:00",content:"Paper Session III: Innovative VR/AR Haptic Sensations",more:"  • “High-Speed Non-Contact Thermal Display Using Infrared Rays and Shutter Mechanism”, Sosuke Ichihashi, Arata Horie, Masaharu Hirose, Zendai Kashino, Shigeo Yoshida, Masahiko Inami\n • “Flowing-Haptic Sleeve: Research on Apparent Tactile Motion Applied to Simulating the Feeling of Flow on the Arm”, Hao-Ping Chien, Ming-Cheng Wu, Chun-Cheng Hsu"},{time:"16:00 ~ 16:20",content:"Coffee Break"},{time:"16:20 ~ 17:20",content:"Panel Discussion: The Future Trends in VR/AR Interactions",more:" Panelists: \n • Andrea Bianchi (KAIST)\n • Liwei Chan (National Yang Ming Chiao Tung University)\n • Min-Chun Hu (National Tsing Hua University)\n • Ray Lee (CORMA New Media)"},{time:"17:20 ~ 17:30",content:"Closing & Best Paper Award Ceremony"}]}},methods:{}},d,!1,function(e){n("2DBd")},"data-v-7d01abb9",null).exports,Contact:n("VU/8")({components:{},data:function(){return{name:"THE VENUE",body:'<p dir="ltr" style="margin-top: 0pt; margin-bottom: 0pt; line-height: 1.7;"><span style="font-size: 19px;">The MIMSVAI 2021 workshop is part of (co-located with) </span><a href="https://www.ubicomp.org/ubicomp2021/" target="blank" style="color: #e89700;">UbiComp 2021↗︎</a><span>, which will be held virtually.</span></p>'}},methods:{}},f,!1,function(e){n("ll8J")},"data-v-d9ab3818",null).exports,Keynotes:n("VU/8")({components:{},data:function(){return{name:"KEYNOTES",speakers:[{img:"http://plopes.org/wp-content/uploads/2019/05/PedroLopes_UChicago2.jpg",name:"Pedro Lopes",agency:"University of Chicago, USA",keynote:"Keynote 1",content:"Human Computer Integration: Powering a New Generation of Interactive Devices",summary:'<p><span style="font-weight: 400;">When we look back to the early days of computing, user and device were distant, often located in separate rooms. Then, in the &rsquo;70s, personal computers &ldquo;moved in&rdquo; with users. In the &rsquo;90s, mobile devices moved computing into users&rsquo; pockets. More recently, wearable devices brought computing into constant physical contact with the user&rsquo;s skin. These transitions proved useful: moving closer to users allowed interactive devices to sense more of their user and act more personal. The main question that drives my research is: </span><strong>what is the next interface paradigm that supersedes wearable devices?</strong></p><p><span style="font-weight: 400;">The primary way researchers have been investigating this is by asking where future interactive devices will be located with respect to the user&rsquo;s body. Many posit that the next generation of interfaces will be implanted inside the user&rsquo;s body. However, I argue that their location with respect to the user&rsquo;s body is not the primary factor; in fact, implanted devices are already happening in that we have pacemakers, insulin pumps, etc. Instead, I argue that the key factor is how will devices </span><strong>integrate</strong><span style="font-weight: 400;"> with the user&rsquo;s biological senses and actuators.&nbsp;</span></p><p><span style="font-weight: 400;">This body-device integration allows us to engineer interactive devices that intentionally borrow parts of the body for input and output, rather than adding more technology to the body. For example, one such type of body-integrated devices, which I have advanced recently, are interactive systems based on electrical muscle stimulation. These devices are able to move their user&rsquo;s muscles using computer-controlled electrical impulses, achieving the functionality of robotic exoskeletons without the bulky motors.&nbsp;</span></p><p><span style="font-weight: 400;">They key insight of my research is that engineering devices that intentionally borrow parts of the user&rsquo;s biology puts forward a </span><strong>new</strong> <strong>generation of miniaturized devices</strong><span style="font-weight: 400;">; allowing us to circumvent traditional physical constrains. For instance, in the case of our devices based on electrical muscle stimulation, they demonstrate how our body-device integration circumvents the constrains imposed by ratio of electrical power and size of a motor (i.e., the stronger/larger a motor is, more current needed to actuate it). Similarly, we demonstrate how our body-device integration approach </span><strong>even allowed us to miniaturize thermal feedback</strong><span style="font-weight: 400;"> (hot/cold sensations) without the need for power-hungry devices like Peltiers, air conditioners or heaters.</span></p>',biography:'<p><a href="https://lab.plopes.org/"><span style="font-weight: 400;color: rgb(17, 85, 204);text-decoration: underline;">Pedro Lopes</span></a><span style="font-weight: 400;"> is an Assistant Professor in Computer Science at the University of Chicago, where he leads the </span><strong>Human Computer Integration lab</strong><span style="font-weight: 400;">. Pedro focuses on </span><strong>integrating computer interfaces with the human body&mdash;exploring the interface paradigm that supersedes wearable computing</strong><span style="font-weight: 400;">. Some of these new integrated-devices include: a device based on muscle stimulation that allows </span><a href="https://www.youtube.com/watch?v=Gz4dphzBb6I"><span style="font-weight: 400;color: rgb(17, 85, 204);    text-decoration: underline;">users to manipulate tools they never seen before</span></a><span style="font-weight: 400;"> or that </span><a href="https://www.youtube.com/watch?v=1BT8REEJibM"><span style="font-weight: 400;color: rgb(17, 85, 204);text-decoration: underline;">accelerate their reaction time</span></a><span style="font-weight: 400;">, or a device that </span><a href="https://www.youtube.com/watch?v=pH68GNkb_fA&amp;feature=youtu.be"><span style="font-weight: 400;color: rgb(17, 85, 204); text-decoration: underline;">leverages the sense of smell to create an illusion of temperature</span></a><span style="font-weight: 400;">. </span><span style="font-weight: 400;">Pedro&rsquo;s work is published at top-tier conferences (ACM CHI &amp; ACM UIST), where he has received four Best Paper awards, two Best Paper nominations and several Best Talk/Demo/Video awards. Pedro&rsquo;s work also captured the interest of media, such as New York Times, MIT Technology Review, NBC, Discovery Channel, NewScientist, Wired and has been shown at Ars Electronica and World Economic Forum (More: </span><a href="https://lab.plopes.org/"><span style="font-weight: 400;color: rgb(17, 85, 204);text-decoration: underline;">https://lab.plopes.org</span></a><span style="font-weight: 400;">)</span></p>'},{img:"https://pbs.twimg.com/profile_images/502132318354419713/UXyxLGjS_400x400.jpeg",name:"Mike Chen",agency:"National Taiwan University, Taiwan",keynote:"Keynote 2",content:"Playing with Perception - Haptics Re-imagined",summary:'<p><span style="font-weight: 400;">Motion simulator was invented over 100 years ago and modern haptic feedback devices were commercialized over 50 years ago. In this talk, we will take a fresh look at the fundamental limitations that have been in place ever since, and re-imaging how these experiences can be perceptually designed to be more realistic, immersive, and enable greater mobility.</span><span style="text-decoration: underline;"><span style="color: #3366ff;"><a style="color: #3366ff;" href="https://dl.acm.org/doi/10.1145/3386569.3392482"> <span style="font-weight: 400;">HeadBlaster</span></a></span></span><span style="font-weight: 400;">, is the first wearable approach to recreating the persistent proprioception and vestibular stimulation during real acceleration, enabling it to provide significantly longer motion sensation than motion platforms.</span><span style="text-decoration: underline;"><span style="color: #3366ff;"><a style="color: #3366ff; text-decoration: underline;" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376292"> MiniatureHaptics</a></span></span><span style="font-weight: 400;"> explores whether haptic feedback can be remapped to just the hand, in order to enable new haptic experiences that are not practical to create at the full, human-body scale.&nbsp;&nbsp;</span></p><p><span style="font-weight: 400;">We will further explore research opportunities in applying perceptual design to haptic feedback, such as a multi-sensory approach that utilizes noise, which is normally undesir</span><span style="font-weight: 400;">able, to enhance haptic experiences, as well as perceptual approaches to address limitations of existing force feedback technologies. Last, we will discuss fun research and design opportunities by shifting focus from reproducing real-world experiences to out-of-this-world experiences. </span></p>',biography:'<p><a href="https://mikechen.com/"><span style="font-weight: 400;"><span style="text-decoration: underline;"><span style="color: #3366ff; text-decoration: underline;">Prof. Mike Y. Chen</span></span></span></a><span style="font-weight: 400;"> creates future user experiences, and innovates at the intersection of HCI, AI, design, perception, and VR/AR. He founded the</span><span style="text-decoration: underline;"><span style="color: #3366ff;"><a style="color: #3366ff;" href="https://ntuhci.org/"> <span style="font-weight: 400;">HCI Lab</span></a></span></span><span style="font-weight: 400;"> at National Taiwan University, which has published at premier ACM conferences including CHI, SIGGRAPH, and UIST, and received Best Talk/Paper and Innovative Game Design Awards. He has</span><span style="text-decoration: underline;"><span style="color: #3366ff; text-decoration: underline;"><a style="color: #3366ff; text-decoration: underline;" href="https://github.com/ntu-hci-lab"> <span style="font-weight: 400;">open-sourced projects</span></a></span></span><span style="font-weight: 400;"> ranging from hardware designs of haptic systems to touchscreen gesture data analytics for ResearchKit, which was mentioned at Apple WWDC 2019.</span></p><p><span style="font-weight: 400;">Mike received his Ph.D. in Computer Science from UC Berkeley, with a Management of Technology (MOT) certificate from Haas School of Business. Prior to NTU, Mike led Mobile R&amp;D at Intel Research and startup Ludic Labs.&nbsp;</span></p><p><span style="font-weight: 400;">For more info: see</span><span style="text-decoration: underline;"><span style="color: #3366ff;"><a style="color: #3366ff;" href="https://mikechen.com/"> <span style="font-weight: 400;">https://mikechen.com/</span></a></span></span></p>'}]}},methods:{}},u,!1,function(e){n("eaNW")},"data-v-2395d821",null).exports,Panel:n("VU/8")({components:{},data:function(){return{name:"PANEL DISCUSSION"}},methods:{}},m,!1,function(e){n("si3c")},"data-v-6cd5e96e",null).exports},data:function(){return{data:"",menuList:"",fbImg:"https://mimsvai.github.io/static/imgs/facebook-loge.png",twitterImg:"https://mimsvai.github.io/static/imgs/twitter-logo.png",fbUrl:" https://www.facebook.com/mimsvai",twUrl:"https://twitter.com/mimsvai",isTop:!1}},mounted:function(){window.addEventListener("scroll",this.handleScroll)},methods:{handleScroll:function(e,t){window.scrollY>300?this.isTop=!1:this.isTop=!0}}},w={render:function(){var e=this.$createElement,t=this._self._c||e;return t("div",{attrs:{id:"app"}},[t("MenuM"),t("section",{attrs:{id:"top","data-aos":"fade-down","data-aos-duration":"2000"}},[t("Intro")],1),t("section",{staticClass:"About section",attrs:{id:"ABOUT","data-aos":"fade-up","data-aos-duration":"1000"}},[t("About")],1),t("section",{staticClass:"Paper section",attrs:{id:"PAPERS"}},[t("Paper")],1),t("section",{staticClass:"Keynotes section",attrs:{id:"KEYNOTES"}},[t("Keynotes")],1),t("section",{staticClass:"Panel section",attrs:{id:"PANEL"}},[t("Panel")],1),t("section",{staticClass:"Agenda section",attrs:{id:"AGENDA"}},[t("Agenda")],1),t("section",{staticClass:"Organizera section",attrs:{id:"ORGANIZER"}},[t("Staff")],1),t("section",{staticClass:"Venue section",attrs:{id:"VENUE"}},[t("Contact")],1),t("div",{staticClass:"socialCantainer",class:{"social-top":this.isTop}},[t("a",{staticClass:"socialIcon fbIcon",style:{"background-image":"url("+this.fbImg+")"},attrs:{href:this.fbUrl,target:"_blank"}}),t("a",{staticClass:"socialIcon twIcon",style:{"background-image":"url("+this.twitterImg+")"},attrs:{href:this.twUrl,target:"_blank"}})])],1)},staticRenderFns:[]};var b=n("VU/8")(y,w,!1,function(e){n("mudV")},null,null).exports,v={components:{},props:["section-name","article-list"],data:function(){return{name:"",data:"",pageStatus:!0,pageDataNum:8,currentPage:1,totalPage:1,firstPage:"第一頁",prev:"上一頁",next:"下一頁",finalPage:"最末頁"}},watch:{currentPage:function(e){this.$router.push({query:{page:e}})},$route:function(){this.name=this.$route.params.PID,this.articleList=this.createList(this.name)}},mounted:function(){null!=this.sectionName?(this.name=this.sectionName,this.pageStatus=!1):this.name=this.$route.params.PID},methods:{createList:function(e){return this.data.filter(function(t){for(var n=0;n<t.categories.length;n++){if(t.categories[n].name==e)return!0}})},changePath:function(e){this.$router.push({path:"/section/"+this.name+"/page/"+e})},pageInit:function(){this.totalPage=Math.ceil(this.articleList.length/this.pageDataNum)+1},setPage:function(e){e<=0||e>this.totalPage||(this.currentPage=e)}}},x={render:function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"newsContainer"},[e._l(e.articleList,function(t,a){return n("div",{staticClass:"newsTitleBlock",on:{click:function(n){return e.changePath(t.id)}}},[n("div",{staticClass:"newsTitle"},[e._v(e._s(t.updated_at.split("T")[0])),n("span",[e._v(e._s(t.title))])]),n("div",{attrs:{id:"underline"}})])}),e.pageStatus?n("div",{staticClass:"pageContainter"},[1!=e.currentPage?n("a",{staticClass:"pageBtn firstPage",attrs:{id:"myhref"},on:{click:function(t){return e.setPage(1)}}},[e._v(e._s(e.firstPage))]):e._e(),1!=e.currentPage?n("a",{staticClass:"pageBtn previous",attrs:{id:"myhref"},on:{click:function(t){return e.setPage(e.currentPage-1)}}},[e._v(e._s(e.prev))]):e._e(),n("select",{directives:[{name:"model",rawName:"v-model",value:e.currentPage,expression:"currentPage"}],attrs:{id:"pageSel"},on:{change:function(t){var n=Array.prototype.filter.call(t.target.options,function(e){return e.selected}).map(function(e){return"_value"in e?e._value:e.value});e.currentPage=t.target.multiple?n:n[0]}}},e._l(e.totalPage,function(t){return n("option",[e._v(e._s(t))])}),0),e.currentPage!=e.totalPage?n("a",{staticClass:"pageBtn next",attrs:{id:"myhref"},on:{click:function(t){return e.setPage(e.currentPage+1)}}},[e._v(e._s(e.next))]):e._e(),e.currentPage!=e.totalPage?n("a",{staticClass:"pageBtn finalPage",attrs:{id:"myhref"},on:{click:function(t){return e.setPage(e.totalPage)}}},[e._v(e._s(e.finalPage))]):e._e()]):e._e()],2)},staticRenderFns:[]};n("VU/8")(v,x,!1,function(e){n("1k2W")},null,null).exports;a.a.use(s.a);var C=new s.a({routes:[{path:"/",name:"HomePage",component:b}]}),k=n("RInU"),T=n.n(k);n("AhPL");a.a.config.productionTip=!1,new a.a({created:function(){T.a.init()},el:"#app",router:C,components:{App:o},template:"<App/>"})},RcHa:function(e,t){},den4:function(e,t){},eaNW:function(e,t){},ll8J:function(e,t){},mudV:function(e,t){},si3c:function(e,t){},t5nL:function(e,t){}},["NHnr"]);
//# sourceMappingURL=app.6051db439a01c7768add.js.map